{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWMWD9RhEtw1nEN0jVEGep",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1182554e828d4c51a9da5b142736e64e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_67f316e9799d437092d3d759ee44d8b7",
              "IPY_MODEL_ae74b926d80a4b33988d7f1348208b0d",
              "IPY_MODEL_461cc9b5a4514be4956aaaa00a17f70e"
            ],
            "layout": "IPY_MODEL_13752679e812469390dadbbad1864a13"
          }
        },
        "67f316e9799d437092d3d759ee44d8b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac39f69a9e5142e4be2f28a501850bd7",
            "placeholder": "​",
            "style": "IPY_MODEL_24c735ff15fb406c9759c546fe6bca0e",
            "value": "100%"
          }
        },
        "ae74b926d80a4b33988d7f1348208b0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0b7429b042f438ba1497c05a62cb002",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_41119627c6494716afd6854bf2593979",
            "value": 10
          }
        },
        "461cc9b5a4514be4956aaaa00a17f70e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7bf9fdc46f294a94b6f82d5831b63abe",
            "placeholder": "​",
            "style": "IPY_MODEL_dcc83a41dd44403eb4d7fbdc5fe6f003",
            "value": " 10/10 [00:47&lt;00:00,  4.48s/it]"
          }
        },
        "13752679e812469390dadbbad1864a13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac39f69a9e5142e4be2f28a501850bd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24c735ff15fb406c9759c546fe6bca0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0b7429b042f438ba1497c05a62cb002": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41119627c6494716afd6854bf2593979": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7bf9fdc46f294a94b6f82d5831b63abe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcc83a41dd44403eb4d7fbdc5fe6f003": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanjaya999/Pytorch/blob/main/bike_or_scooter_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQgaLIal0tK7",
        "outputId": "a2223b72-2c83-4a4d-9f98-0b457fc593e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "creating data directory\n",
            "downloading bike scooter image\n",
            "unzipping bikeScooter\n"
          ]
        }
      ],
      "source": [
        "#get data\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device\n",
        "import requests\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "data_path = Path(\"data/\")\n",
        "image_path = data_path\n",
        "\n",
        "#if image folder doent exist , download it and prepare\n",
        "\n",
        "if image_path.is_dir():\n",
        "  print(f\"{image_path} directory already exist\")\n",
        "else:\n",
        "  print(f\"creating {image_path} directory\")\n",
        "  image_path.mkdir(parents=True,exist_ok=True)\n",
        "\n",
        "#download test file\n",
        "with open(data_path / \"data2.zip\" , \"wb\") as f:\n",
        "  request= requests.get(\"https://github.com/sanjaya999/Pytorch/raw/refs/heads/main/data2.zip\")\n",
        "  print(\"downloading bike scooter image\")\n",
        "  f.write(request.content)\n",
        "\n",
        "#unzip\n",
        "with zipfile.ZipFile(data_path/ \"data2.zip\" , \"r\") as zip_ref:\n",
        "  print(\"unzipping bikeScooter\")\n",
        "  zip_ref.extractall(image_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir='/content/data/data2/train/'\n",
        "test_dir ='/content/data/data2/test/'"
      ],
      "metadata": {
        "id": "26Cw2siV0v4D"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image, UnidentifiedImageError\n",
        "import os\n",
        "\n",
        "def clean_bad_images(folder):\n",
        "    for subdir, _, files in os.walk(folder):\n",
        "        for file in files:\n",
        "            filepath = os.path.join(subdir, file)\n",
        "            try:\n",
        "                with Image.open(filepath) as img:\n",
        "                    img.verify()\n",
        "            except (UnidentifiedImageError, OSError):\n",
        "                print(f\"Removing corrupted file: {filepath}\")\n",
        "                os.remove(filepath)\n",
        "\n",
        "# Clean your dataset\n",
        "clean_bad_images(train_dir)\n",
        "clean_bad_images(test_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOJPJJ8k3fff",
        "outputId": "2359e368-526e-4249-f522-79fdec5a5e8d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing corrupted file: /content/data/data2/train/bike/Bajaj Bikes in India  Price, New Models 2025, Mile_4.jpg\n",
            "Removing corrupted file: /content/data/data2/train/bike/KTM Bikes Price - Check Images, Reviews & Specs in_1.jpg\n",
            "Removing corrupted file: /content/data/data2/train/bike/Quality, High-Performance suzuki bike price - Alib.jpg\n",
            "Removing corrupted file: /content/data/data2/train/bike/Yamaha Bikes - Latest Price List 2025, New Models _5.jpg\n",
            "Removing corrupted file: /content/data/data2/train/bike/TVS Bikes Price in India - New TVS Models 2025, Im_3.jpg\n",
            "Removing corrupted file: /content/data/data2/test/bike/KTM Bikes Price (March Offers!), Images, Reviews a.png\n",
            "Removing corrupted file: /content/data/data2/test/bike/Yamaha Bikes in India  Price, New Models 2025, Mil_4.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MYmqjnWT3fiK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lSmfhUq63fk_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##transform data into tensors\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets , transforms\n",
        "\n",
        "#transforming data with torchvision.transforms\n",
        "#write a transform for image\n",
        "\n",
        "data_transform = transforms.Compose([\n",
        "    #resize image to 64x64\n",
        "    transforms.Resize(size =(64,64)),\n",
        "\n",
        "    #flip the image horizontally 50% of the time\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "\n",
        "    #convert image to tensor\n",
        "    transforms.ToTensor()\n",
        "\n",
        "])"
      ],
      "metadata": {
        "id": "TzrVAZOq08Wg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#use image folder to create datasets\n",
        "\n",
        "from torchvision import datasets\n",
        "train_data = datasets.ImageFolder(root = train_dir,\n",
        "                                 transform = data_transform\n",
        "                                )\n",
        "\n",
        "test_data = datasets.ImageFolder(root = test_dir,\n",
        "                                transform = data_transform\n",
        "                                )\n",
        "\n",
        "test_data,train_data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvFeVLxW1F1H",
        "outputId": "6050aa1d-da29-41b1-888b-ab7f30fab1e8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Dataset ImageFolder\n",
              "     Number of datapoints: 27\n",
              "     Root location: /content/data/data2/test/\n",
              "     StandardTransform\n",
              " Transform: Compose(\n",
              "                Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=True)\n",
              "                RandomHorizontalFlip(p=0.5)\n",
              "                ToTensor()\n",
              "            ),\n",
              " Dataset ImageFolder\n",
              "     Number of datapoints: 272\n",
              "     Root location: /content/data/data2/train/\n",
              "     StandardTransform\n",
              " Transform: Compose(\n",
              "                Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=True)\n",
              "                RandomHorizontalFlip(p=0.5)\n",
              "                ToTensor()\n",
              "            ))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get class name as list\n",
        "classname = train_data.classes\n",
        "classname_dict = train_data.class_to_idx\n",
        "print(classname)\n",
        "print(classname_dict)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhQR8M4b1Mjj",
        "outputId": "8cb88d03-41f2-4d2d-bb9c-cb9939f26a7f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['bike', 'scooter']\n",
            "{'bike': 0, 'scooter': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AmQiGFvq5mfy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5cEYNnYk5mnt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(dataset=train_data,\n",
        "                              batch_size= 32,\n",
        "                              shuffle=True)\n",
        "\n",
        "test_dataloader = DataLoader(dataset=test_data,\n",
        "                              batch_size= 32,\n",
        "                              shuffle=False)\n",
        "print(len(train_data) , len(test_data))\n",
        "\n",
        "train_dataloader , test_dataloader\n",
        "#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVMSdyfV1Qcp",
        "outputId": "f6f05818-54f6-41ec-b674-1dc12bf5b43f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "272 27\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x79f25e4e8510>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x79f267bc4190>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch, (X, y) in enumerate(test_dataloader):\n",
        "  print(batch, X[0].shape, y)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlxp-gv57ADJ",
        "outputId": "7a38f081-0f4e-4ba9-c25e-b31324d1dc60"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 torch.Size([3, 64, 64]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch, (X, y) in enumerate(train_dataloader):\n",
        "  print(batch, X[0].shape, y)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfWC5s6M9nx1",
        "outputId": "a653e565-bb37-4ab8-a39f-77d6f60e35a4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 torch.Size([3, 64, 64]) tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,\n",
            "        1, 0, 1, 0, 0, 1, 0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# list(train_dataloader_simple)"
      ],
      "metadata": {
        "id": "fjkPFFR_6VOr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simple_transform = transforms.Compose([\n",
        "    transforms.Resize(size=(64,64)),\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "LJfU-vao1Xrd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "EgCsIT9T2LNp",
        "outputId": "4ad070ac-85b8-4fb9-e8fb-a9467bc5b4b6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/data/data2/train/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load and transform data\n",
        "\n",
        "train_data_simple = datasets.ImageFolder(root=train_dir,\n",
        "                                         transform=simple_transform,\n",
        "                                         )\n",
        "test_data_simple = datasets.ImageFolder(root=test_dir, transform=simple_transform,)\n",
        "\n",
        "#setup batch size and number of worker\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS =os.cpu_count()\n",
        "\n",
        "train_dataloader_simple = DataLoader(dataset=train_data_simple,\n",
        "                                     batch_size=BATCH_SIZE,\n",
        "                                     num_workers=NUM_WORKERS,\n",
        "                                     shuffle=True)\n",
        "\n",
        "test_dataloader_simple = DataLoader(dataset=test_data_simple,\n",
        "                                    batch_size=BATCH_SIZE,\n",
        "                                    num_workers=NUM_WORKERS,\n",
        "                                    shuffle=False)\n",
        "\n",
        "train_dataloader_simple , test_dataloader_simple"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRo21Jqk1emz",
        "outputId": "d4359401-885c-448f-a69e-c28deaa00662"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x79f25e4e59d0>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x79f25e535950>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a model\n",
        "\n",
        "class TinyVGG(nn.Module):\n",
        "  def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
        "    super().__init__()\n",
        "    self.conv_block_1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=input_shape,\n",
        "                    out_channels=hidden_units,\n",
        "                    kernel_size=3,\n",
        "                    stride=1,\n",
        "                    padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                    out_channels=hidden_units,\n",
        "                    kernel_size=3,\n",
        "                    stride=1,\n",
        "                    padding=1),\n",
        "         nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2,\n",
        "                      stride=2)\n",
        "    )\n",
        "    self.conv_block_2= nn.Sequential(\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                    out_channels=hidden_units,\n",
        "                    kernel_size=3,\n",
        "                    stride=1,\n",
        "                    padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                    out_channels=hidden_units,\n",
        "                    kernel_size=3,\n",
        "                    stride=1,\n",
        "                    padding=1),\n",
        "         nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2,\n",
        "                      stride=2)\n",
        "    )\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=hidden_units*16*16 ,out_features=output_shape),\n",
        "        # nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    x = self.conv_block_1(x)\n",
        "    x = self.conv_block_2(x)\n",
        "    x = self.classifier(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "YFfzr5OM1l0G"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "126gfU9J6EeA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0 = TinyVGG(input_shape=3,\n",
        "                  hidden_units=3,\n",
        "                  output_shape=2)\n",
        "model_0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xfd7i60j1o3O",
        "outputId": "d5cd02da-f038-41dd-cbca-830c9ad26834"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TinyVGG(\n",
              "  (conv_block_1): Sequential(\n",
              "    (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv_block_2): Sequential(\n",
              "    (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(model: torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer):\n",
        "    # Put model in train mode\n",
        "    model.train()\n",
        "\n",
        "    # Setup train loss and train accuracy values\n",
        "    train_loss, train_acc = 0, 0\n",
        "\n",
        "    # Loop through data loader data batches\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Send data to target device\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        # print(X.shape, y.shape)\n",
        "\n",
        "        # 1. Forward pass\n",
        "        y_pred = model(X)\n",
        "        # 2. Calculate  and accumulate loss\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # 3. Optimizer zero grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 4. Loss backward\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate and accumulate accuracy metrics across all batches\n",
        "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "        # print(y_pred_class)\n",
        "        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
        "\n",
        "    # Adjust metrics to get average loss and accuracy per batch\n",
        "    train_loss = train_loss / len(dataloader)\n",
        "    train_acc = train_acc / len(dataloader)\n",
        "    # print(train_loss, train_acc)\n",
        "    return train_loss, train_acc"
      ],
      "metadata": {
        "id": "Hdo66fIR1rVc"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(model: torch.nn.Module,\n",
        "              dataloader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module):\n",
        "    # Put model in eval mode\n",
        "    model.eval()\n",
        "\n",
        "    # Setup test loss and test accuracy values\n",
        "    test_loss, test_acc = 0, 0\n",
        "\n",
        "    # Turn on inference context manager\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        # Loop through DataLoader batches\n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "            # Send data to target device##\n",
        "            # print(X, y)\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            # 1. Forward pass\n",
        "            test_pred_logits = model(X)\n",
        "\n",
        "            # 2. Calculate and accumulate loss\n",
        "            loss = loss_fn(test_pred_logits, y)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            # Calculate and accumulate accuracy\n",
        "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
        "            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
        "\n",
        "    # Adjust metrics to get average loss and accuracy per batch\n",
        "    test_loss = test_loss / len(dataloader)\n",
        "    test_acc = test_acc / len(dataloader)\n",
        "    return test_loss, test_acc"
      ],
      "metadata": {
        "id": "GG5T844E187i"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "# 1. Take in various parameters required for training and test steps\n",
        "def train(model: torch.nn.Module,\n",
        "          train_dataloader: torch.utils.data.DataLoader,\n",
        "          test_dataloader: torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n",
        "          epochs: int = 5):\n",
        "\n",
        "    # 2. Create empty results dictionary\n",
        "    results = {\"train_loss\": [],\n",
        "        \"train_acc\": [],\n",
        "        \"test_loss\": [],\n",
        "        \"test_acc\": []\n",
        "    }\n",
        "\n",
        "    # 3. Loop through training and testing steps for a number of epochs\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        train_loss, train_acc = train_step(model=model,\n",
        "                                           dataloader=train_dataloader,\n",
        "                                           loss_fn=loss_fn,\n",
        "                                           optimizer=optimizer)\n",
        "        test_loss, test_acc = test_step(model=model, dataloader=test_dataloader,loss_fn=loss_fn)\n",
        "\n",
        "        # 4. Print out what's happening\n",
        "        print(\n",
        "            f\"Epoch: {epoch+1} | \"\n",
        "            f\"train_loss: {train_loss:.4f} | \"\n",
        "            f\"train_acc: {train_acc:.4f} | \"\n",
        "            f\"test_loss: {test_loss:.4f} | \"\n",
        "            f\"test_acc: {test_acc:.4f}\"\n",
        "        )\n",
        "\n",
        "        # 5. Update results dictionary\n",
        "        # Ensure all data is moved to CPU and converted to float for storage\n",
        "        results[\"train_loss\"].append(train_loss.item() if isinstance(train_loss, torch.Tensor) else train_loss)\n",
        "        results[\"train_acc\"].append(train_acc.item() if isinstance(train_acc, torch.Tensor) else train_acc)\n",
        "        results[\"test_loss\"].append(test_loss.item() if isinstance(test_loss, torch.Tensor) else test_loss)\n",
        "        results[\"test_acc\"].append(test_acc.item() if isinstance(test_acc, torch.Tensor) else test_acc)\n",
        "\n",
        "    # 6. Return the filled results at the end of the epochs\n",
        "    return results\n",
        "\n",
        "  # Setup loss function and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=model_0.parameters(), lr=0.001)\n",
        "\n",
        "model_0_results = train(model=model_0,\n",
        "                        train_dataloader=train_dataloader_simple,\n",
        "                        test_dataloader=test_dataloader_simple,\n",
        "                        optimizer=optimizer,\n",
        "                        loss_fn=loss_fn,\n",
        "                        epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570,
          "referenced_widgets": [
            "1182554e828d4c51a9da5b142736e64e",
            "67f316e9799d437092d3d759ee44d8b7",
            "ae74b926d80a4b33988d7f1348208b0d",
            "461cc9b5a4514be4956aaaa00a17f70e",
            "13752679e812469390dadbbad1864a13",
            "ac39f69a9e5142e4be2f28a501850bd7",
            "24c735ff15fb406c9759c546fe6bca0e",
            "a0b7429b042f438ba1497c05a62cb002",
            "41119627c6494716afd6854bf2593979",
            "7bf9fdc46f294a94b6f82d5831b63abe",
            "dcc83a41dd44403eb4d7fbdc5fe6f003"
          ]
        },
        "id": "F2_UA9WV1uCF",
        "outputId": "4a4842c8-8f72-4fc3-86e0-c13ba0942e3e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1182554e828d4c51a9da5b142736e64e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | train_loss: 0.6781 | train_acc: 0.5868 | test_loss: 0.7120 | test_acc: 0.4815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2 | train_loss: 0.6827 | train_acc: 0.5729 | test_loss: 0.7124 | test_acc: 0.4815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3 | train_loss: 0.6793 | train_acc: 0.5833 | test_loss: 0.7120 | test_acc: 0.4815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4 | train_loss: 0.6781 | train_acc: 0.5868 | test_loss: 0.7122 | test_acc: 0.4815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5 | train_loss: 0.6826 | train_acc: 0.5729 | test_loss: 0.7122 | test_acc: 0.4815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 6 | train_loss: 0.6781 | train_acc: 0.5868 | test_loss: 0.7121 | test_acc: 0.4815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 7 | train_loss: 0.6792 | train_acc: 0.5833 | test_loss: 0.7122 | test_acc: 0.4815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 8 | train_loss: 0.6792 | train_acc: 0.5833 | test_loss: 0.7124 | test_acc: 0.4815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 9 | train_loss: 0.6804 | train_acc: 0.5799 | test_loss: 0.7125 | test_acc: 0.4815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10 | train_loss: 0.6747 | train_acc: 0.5972 | test_loss: 0.7124 | test_acc: 0.4815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RrqXvZOn3FQS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}